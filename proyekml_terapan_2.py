# -*- coding: utf-8 -*-
"""ProyekML_TERAPAN-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17oXOMrBKBz3MTLQZVsbxgWcApRsbN2HD

# Proyek Akhir Kelas Machine Learning Terapan - Mohamed

## Menginstall library LightFM untuk membuat model sistem rekomendasi

## Memanggil library yang dibutuhkan
"""

!pip install lightfm

import kagglehub
import os
import pandas as pd
import matplotlib.pyplot as plt
from lightfm import LightFM
from lightfm.data import Dataset
from lightfm.cross_validation import random_train_test_split
from lightfm.evaluation import precision_at_k, recall_at_k
import numpy as np
from sklearn.preprocessing import OneHotEncoder

"""# Data Loading"""

path = kagglehub.dataset_download("arashnic/book-recommendation-dataset")

print(path)

data_path = os.listdir(path)

print(data_path)

ratings_df = pd.read_csv(f'/{path}/{data_path[0]}')
books_df = pd.read_csv(f'/{path}/{data_path[3]}')
users_df = pd.read_csv(f'/{path}/{data_path[1]}')

"""Terdapa tiga file dataset yang akan digunakan, yaitu :


1.   Ratings.csv
2.   Users.csv
3.   Books.csv

Berikutnya akan dilakukan Exploratory Data Analysis (EDA) untuk mengetahui karakteristik dari data.

#Exploratory Data Analysis

## Melihat 5 data teratas dari dataset ratings
"""

ratings_df.head()

"""Tabel diatas merupakan contoh dari bentuk data yang berada di dalam dataset ratings_df.

## Melihat 5 data teratas dari dataset books
"""

books_df.head()

"""Tabel diatas merupakan contoh dari bentuk data yang berada di dalam dataset books_df.

## Melihat 5 data teratas dari dataset users
"""

users_df.head()

"""Tabel diatas merupakan contoh dari bentuk data yang berada di dalam dataset users_df.

## Membuat fungsi untuk melihat informasi mengenai dataset, statistika deskriptif serta melihat jumlah data null dan jumlah data duplikat.
"""

def get_df_info(df):
  print("-"*50)
  print(df.info())
  print("-"*50)
  print(df.describe(include='all'))
  print("-"*50)
  print(f'Jumlah data null :\n{df.isnull().sum()}')
  print("-"*50)
  print(f'Jumlah data duplikat :{df.duplicated().sum()}')
  print("-"*50)

"""### Melihat informasi terkait ratings_df beserta statistika deskriptif dan jumlah mssing value dan jumlah data duplikat"""

get_df_info(ratings_df)

"""Tidak ditemukan kolom dengan missing value dan tidak ditemukan data duplikat.

### Melihat informasi terkait books_df beserta statistika deskriptif dan jumlah missing value dan jumlah data duplikat
"""

get_df_info(books_df)

"""Ditemukan 7 missing value pada DataFrame books_df, 2 pada kolom Book-Author , 2 pada kolom Publisher, dan dan 3 pada kolom Image-URL-L.

### Melihat informasi terkait users_df beserta statistika deskriptif dan jumlah mssing value dan jumlah data duplikat
"""

get_df_info(users_df)

"""Ditemukan 110762 missing value pada kolom Age

## Melihat distribusi dari data Age pada dataset users.
"""

plt.hist(x = users_df['Age'] , bins = 10)
plt.title('Distribusi umur pengguna')
plt.xlabel('Umur')
plt.ylabel('Jumlah pengguna')
plt.show()

users_df['Age'].skew()

"""Distribusi data pada kolom Age cenderung *right skew*, dibuktikan dengan nilai skew sebesar 1.178391610309946

### Melihat distribusi data pada kolom Book-Rating
"""

plt.hist(x = ratings_df['Book-Rating'] , bins=10)
plt.title('Distribusi rating buku')
plt.xlabel('Rating')
plt.ylabel('Jumlah buku')
plt.show()

ratings_df['Book-Rating'].skew()

"""Terdapat ketidakseimbangan dalam distribusi nilai , dimana rating 0 berjumlah 700 ribu mendominasi data rating lain. hal tersebut membuat distribusi data menjadi cenderung ke *right skew* dibuktikan dengan nilai skewness sebesar 0.733641207848559, sehingga agar dapat menyeimbangkan nilainya data dari rating 0 perlu dihapus.

### Melihat 5 tahun dengan jumlah publikasi terbanyak
"""

books_df['Year-Of-Publication'].value_counts().head()

"""Tahun 2002 menjadi tahun dimana banyak buku yang dipublikasikan , dibuktikan dengan jumlah publikasi terbanyak pada angka 919 buku, disusul dengan tahun 2001  dengan buku yang dipublikasikan sebanyak 8115 buku, tahun 2000 dengan buku yang dipublikasikan sebanyak 787 buku, tahun 2003  dengan buku yang dipublikasikan sebanyak 731, dan tahun 1999 dengan buku yang dipublikasikan sebanyak	728 buku.

### Melihat 5 penerbit dengan jumlah terbitan terbanyak
"""

books_df['Publisher'].value_counts().head()

"""Ballantine Books merupakan Publisher urutan teratas dalam hal penerbitan buku, terdapat 300 buku yang telah diterbitkan olehnya, di bawahnya adalah Pocket dengan 255 buku yang diterbitkan, dibawahnya lagi ada Berkley Publishing Group dengan 232 buku yang diterbitkan, dibawahnya lagi ada Warner Books dengan 203 buku yang diterbitkan, dan dibawahnya lagi ada Bantams Books dengan 202 buku yang diterbitkan.

### Hasil Akhir


1.   Pada file ratings.csv, terdapat total 1149780 baris data dengan 3 kolom data, tidak terdapat missing value dan tidak memiliki data duplikat sehingga tidak dibutuhkan tindakan imputasi nilai maupun penghapusan nilai duplikat.

2.   Pada file books.csv, terdapat total 271360 baris data dengan 8 kolom data, tidak terdapat missing value namun terdapat 7 data duplikat sehingga perlu untuk menghapus data duplikat tersebut.

3.  Pada file users.csv, terdapat total 278858 baris data dengan 3 kolom data, terdapat 110762 missing value pada kolom age namun tidak memiliki data duplikat sehingga diperlukan imputasi data terhadap kolom age menggunakan nilai median dari kolom age dikarenakan distribusi data dari kolom age yang cenderung *right skew* dibuktikan dengan nilai *skewness* sebesar 1.17 yang menandakan bahwa distribusi data cenderung mengalami *skewness*

4. Tahun 2002 menjadi tahun dimana banyak buku yang dipublikasikan , dibuktikan dengan jumlah publikasi terbanyak pada angka 919 buku, disusul dengan tahun 2001  dengan buku yang dipublikasikan sebanyak 8115 buku, tahun 2000 dengan buku yang dipublikasikan sebanyak 787 buku, tahun 2003  dengan buku yang dipublikasikan sebanyak 731, dan tahun 1999 dengan buku yang dipublikasikan sebanyak	728 buku.

5. Ballantine Books merupakan Publisher urutan teratas dalam hal penerbitan buku, terdapat 300 buku yang telah diterbitkan olehnya, di bawahnya adalah Pocket dengan 255 buku yang diterbitkan, dibawahnya lagi ada Berkley Publishing Group dengan 232 buku yang diterbitkan, dibawahnya lagi ada Warner Books dengan 203 buku yang diterbitkan, dan dibawahnya lagi ada Bantams Books dengan 202 buku yang diterbitkan.

## Data Preparation

### Menghapus data anomali pada kolom Year-of-Publication dalam DataFrame books_df berupa tahun ke-0
"""

books_df = books_df[books_df['Year-Of-Publication'] != 0]

"""Berhasil menghapus data anomali berupa tahun ke 0 dari DataFrame books_df pada kolom Year-Of-Publication

### Melakukan imputasi data pada kolom Age dengan nilai median
"""

users_df.fillna(users_df['Age'].median() , inplace=True)

"""Imputasi data pada kolom age menggunakan nilai median dilakukan dikarenakan distribusi data dari kolom age yang cenderung *right skew*

### Menghapus nilai duplikat dari DataFrame books_df
"""

books_df.drop_duplicates(inplace=True)

"""Penghapusan dari nilai duplikat dalam DataFrame books_df dilakukan dikarenakan jumlah data duplikat yang sedikit dan tidak berpengaruh secara signifikan bilamana dihapus serta membuat data menjadi lebih bersih dan siap untuk dijadikan data latih untuk model.

### Menghapus kolom yang tidak diperlukan pada DataFrame books_df
"""

books_df.drop(['Image-URL-S' , 'Image-URL-M' , 'Image-URL-L'], inplace=True , axis=1)

"""Penghapusan kolom 'Image-URL-S' , 'Image-URL-M' , 'Image-URL-L' dilakukan dikarenakan ketiga kolom tersebut tidak dibutuhkan dan tidak digunakan dalam melatih model.

### Menyaring data dari kolom Book-Rating
"""

ratings_df = ratings_df[ratings_df['Book-Rating'] >= 1]

"""Menyaring data dari DataFrame ratings_df pada kolom Book-Rating untuk mengambil rating sama dengan atau lebih dari 1 untuk menghindari noise pada data latih dan menghindari distorsi pada model.

### Melakukan sampling data sebanyak 10001 untuk masing-masing DataFame
"""

users_df = users_df[:10001]
books_df = books_df[:10001]
ratings_df = ratings_df[:10001]

"""Pembatasan data hingga 10001 pada setiap DataFrame dilakukan untuk mempercepat proses pelatihan model  serta menghemat sumber daya.

### Merging DataFrame ratings_df , users_df , dan books_df
"""

merged_df = pd.merge(ratings_df , users_df , on='User-ID')
merged_df = pd.merge(merged_df , books_df , on='ISBN')

merged_df.head()

"""Merge data ini dilakukan biar semua info penting—rating, data user, dan detail buku—nyatu di satu DataFrame. Jadi, model bisa belajar pola dari hubungan antar user dan buku secara lebih lengkap, cocok buat sistem rekomendasi, baik collaborative maupun content-based.

### Melakukan proses encoding menggunakan OneHotEncoder() pada DataFrame merged_df pada kolom [ 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher']
"""

items_df = merged_df[['Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher']]
encoder = OneHotEncoder()
encoded_df = encoder.fit_transform(items_df)

"""Proses encoding data pada merged_df dilakukan agar dapat memudahkan model dalam menemukan pola tersembunyi pada data serta mempercepat konvergensi model

### Menyiapkan data latih untuk model sistem rekomendasi dengan teknik Collaborative Filtering
"""

dataset1 = Dataset()
dataset1.fit(merged_df['User-ID'] , merged_df['ISBN'])

(interactions_1, weights_1) = dataset1.build_interactions([(row['User-ID'], row['ISBN'], row['Book-Rating']) for _, row in merged_df.iterrows()])

"""fitting pada dataset1 menggunakan data pada kolom User-ID dan ISBN dilakukan untuk membuat dataset yang nantinya akan digunakan dalam proses pelatihan pada model sistem rekomendasi collaborative filtering

### Menyiapkan data latih untuk model sistem rekomendasi dengan teknik Content-Based Filtering
"""

item_features_list = [
    (row['ISBN'],[row['Book-Title'] , row['Book-Author'], row['Year-Of-Publication'], row['Publisher']])
    for _, row in merged_df.iterrows()
]

dataset2 = Dataset()
all_features = pd.unique(
    merged_df[['Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher']]
    .values
    .ravel()
)
dataset2.fit(merged_df['User-ID'] , merged_df['ISBN'])
dataset2.fit_partial(items=merged_df['ISBN'] , item_features =  all_features)

item_features = dataset2.build_item_features(item_features_list)

(interactions_2, weights_2) = dataset2.build_interactions([(row['User-ID'], row['ISBN'], row['Book-Rating']) for _, row in merged_df.iterrows()])

"""fitting pada dataset2 menggunakan data pada kolom User-ID dan ISBN dan fit_partial pada dataset2 menggunakan data buku melalui perantara kolom ISBN dilakukan untuk membuat dataset yang nantinya akan digunakan dalam proses pelatihan pada model sistem rekomendasi content-based filtering

### Hasil Akhir

1.   Berhasil menghapus data anomali berupa tahun ke 0 dari DataFrame books_df pada kolom Year-Of-Publication
2.  Imputasi data pada kolom age menggunakan nilai median dilakukan dikarenakan distribusi data dari kolom age yang cenderung *right skew*
3. Penghapusan dari nilai duplikat dalam DataFrame books_df dilakukan dikarenakan jumlah data duplikat yang sedikit dan tidak berpengaruh secara signifikan bilamana dihapus serta membuat data menjadi lebih bersih dan siap untuk dijadikan data latih untuk model.
4. Penghapusan kolom 'Image-URL-S' , 'Image-URL-M' , 'Image-URL-L' dilakukan dikarenakan ketiga kolom tersebut tidak dibutuhkan dan tidak digunakan dalam melatih model.
5. Menyaring data dari DataFrame ratings_df pada kolom Book-Rating untuk mengambil rating sama dengan atau lebih dari 1 untuk menghindari noise pada data latih dan menghindari distorsi pada model.
6. Pembatasan data hingga 10001 pada setiap DataFrame dilakukan untuk mempercepat proses pelatihan model  serta menghemat sumber daya.
7. Merge data ini dilakukan biar semua info penting—rating, data user, dan detail buku—nyatu di satu DataFrame. Jadi, model bisa belajar pola dari hubungan antar user dan buku secara lebih lengkap, cocok buat sistem rekomendasi, baik collaborative maupun content-based.
8. Proses encoding data pada merged_df dilakukan agar dapat memudahkan model dalam menemukan pola tersembunyi pada data serta mempercepat konvergensi model
9. fitting pada dataset1 menggunakan data pada kolom User-ID dan ISBN dilakukan untuk membuat dataset yang nantinya akan digunakan dalam proses pelatihan pada model sistem rekomendasi collaborative filtering
10. fitting pada dataset2 menggunakan data pada kolom User-ID dan ISBN dan fit_partial pada dataset2 menggunakan data buku melalui perantara kolom ISBN dilakukan untuk membuat dataset yang nantinya akan digunakan dalam proses pelatihan pada model sistem rekomendasi content-based filtering

## Modeling

### Melatih model sistem rekomendasi dengna teknik Collaborative Filtering
"""

# Commented out IPython magic to ensure Python compatibility.
model1 = LightFM(loss='warp-kos')
# %time model1.fit(interactions_1, epochs=150 , num_threads=4)

"""Model dilatih dengan menggunakan dataset interactions_1 dengan 150 epochs

### Melatih model sistem rekomendasi dengna teknik Content-Based Filtering
"""

# Commented out IPython magic to ensure Python compatibility.
model2 = LightFM(loss='warp-kos')
# %time model2.fit(interactions_2, epochs=150 , num_threads=4, item_features=encoded_df)

"""Model dilatih dengan menggunakan dataset interactions_2 dengan 150 epochs dengan tambahan item_features untuk melatih model sistem rekomendasi dengan teknik Content-Based Filtering

### Membuat fungsi untuk melakukan prediksi dan memberikan sistem rekomendasi
"""

def get_prediction(dataset , interactions, model , user_id):
  user_id_map, user_feature_map, item_id_map, item_feature_map = dataset.mapping()

  user_internal_id = user_id in user_id_map
  if not user_internal_id:
    print(user_id_map)
    print("User tidak ditemukan")
    return


  n_items = interactions.shape[1]
  scores = model.predict(user_internal_id, np.arange(n_items))

  # Recommend top 5 items
  top_items = np.argsort(-scores)[:5]

  # Get ISBNs back from internal IDs
  reverse_item_map = {v: k for k, v in item_id_map.items()}
  recommended_isbns = [reverse_item_map[item_id] for item_id in top_items]


  books_data = books_df[books_df['ISBN'].isin(recommended_isbns)]
  return books_data

"""fungsi ini dibuat untuk melakukan prediksi menggunakan model untuk seorang pelanggan menggunakan user_id

### Melakukan prediksi menggunakan model sistem rekomendasi dengan teknik collaborative filtering dengan user_id = 730
"""

get_prediction(dataset1, interactions_1 , model1 , 730 )

"""### Melakukan prediksi menggunakan model sistem rekomendasi dengan teknik content-based filtering dengan user_id = 730"""

get_prediction(dataset2, interactions_2 , model2 , 730)

"""## Evaluasi Model

### Membuat fungsi untuk melakukan proses evaluasi terhadap model menggunakan metrik Precision@K dan Recall@K
"""

def evaluate_model_at_k(k=2):
  print('*'*50)
  print("Collaborative Filtering")
  print("Precision@K :",precision_at_k(model1, interactions_1, k=k).mean())
  print("Recall@K :",recall_at_k(model1,interactions_1,k=k).mean())
  print('*'*50)
  print("Content-based Filtering")
  print("Precision@K :",precision_at_k(model2, interactions_2,item_features=encoded_df, k=k).mean())
  print("Recall@K :",recall_at_k(model2,interactions_2,item_features=encoded_df,k=k).mean())
  print('*'*50)

"""fungsi ini dibuat untuk melakukan evaluasi terhadap tiap model dengan lebih mudah"""

evaluate_model_at_k()

evaluate_model_at_k(k = 5)

evaluate_model_at_k(k = 10)

"""### Collaborative Filtering (LightFM + WARP-KOS)
Model ini menggunakan interaksi historis antara pengguna dan buku dalam bentuk implicit feedback (misalnya rating), tanpa menggunakan informasi konten buku. LightFM digunakan sebagai algoritma dengan loss function WARP-KOS yang mengutamakan performa ranking.

$$\hat{r}_{u,i} = \mathbf{p}_u^\top \mathbf{q}_i + b_u + b_i$$

**Keterangan:**

* $\hat{r}_{u,i}$ = skor prediksi untuk interaksi antara user $u$ dan item $i$
* $\mathbf{p}_u$ = vektor representasi laten untuk user $u$
* $\mathbf{q}_i$ = vektor representasi laten untuk item $i$
* $b_u$ = bias untuk user $u$
* $b_i$ = bias untuk item $i$

---

Loss function WARP-KOS (Weighted Approximate-Rank Pairwise - k-Order Statistic):

![WARP-KOS Loss](https://latex.codecogs.com/png.image?\dpi{150}&space;\mathcal{L}=-\sum_{(u,i,j)}\log\sigma(\hat{r}_{u,i}-\hat{r}_{u,j}))



**Keterangan:**

* $\mathcal{L}$ = total loss yang diminimalkan selama training
* $\sigma$ = fungsi sigmoid
* $\hat{r}_{u,i}$ = skor prediksi user $u$ terhadap item positif $i$
* $\hat{r}_{u,j}$ = skor prediksi user $u$ terhadap item negatif $j$
* $i \in \text{positive items}$, $j \in \text{negative samples}$

WARP-KOS bekerja dengan men-sampling item negatif $j$ hingga ditemukan satu yang skor prediksinya lebih tinggi dari item positif $i$, lalu menghitung loss berdasarkan perbedaan skor.

#### Kelebihan

* Efektif untuk data dengan interaksi yang sparse
* Tidak memerlukan metadata atau fitur konten tambahan
* Fokus pada peningkatan ranking item relevan

#### Kekurangan

* Tidak dapat menangani cold-start item atau user baru
* Bergantung pada interaksi historis yang cukup untuk performa optimal

---

### Content-Based Filtering (LightFM + WARP-KOS)

Model ini juga menggunakan LightFM dengan WARP-KOS, namun ditambahkan fitur konten dari item seperti judul, penulis, atau kategori buku. Fitur-fitur ini digunakan untuk membuat representasi tambahan item sehingga memungkinkan sistem merekomendasikan buku berdasarkan kemiripan konten.

$$ \hat{r}_{u,i} = \mathbf{p}_u^\top \mathbf{q}_i + b_u + b_i $$

**Keterangan:**

* $\hat{r}_{u,i}$ = skor prediksi untuk interaksi antara user $u$ dan item $i$
* $\mathbf{p}_u$ = vektor representasi laten untuk user $u$
* $\mathbf{q}_i$ = vektor representasi laten untuk item $i$
* $b_u$ = bias untuk user $u$
* $b_i$ = bias untuk item $i$

---

Loss function WARP-KOS (Weighted Approximate-Rank Pairwise - k-Order Statistic):

![WARP-KOS Loss](https://latex.codecogs.com/png.image?\dpi{150}&space;\mathcal{L}=-\sum_{(u,i,j)}\log\sigma(\hat{r}_{u,i}-\hat{r}_{u,j}))



**Keterangan:**

* $\mathcal{L}$ = total loss yang diminimalkan selama training
* $\sigma$ = fungsi sigmoid
* $\hat{r}_{u,i}$ = skor prediksi user $u$ terhadap item positif $i$
* $\hat{r}_{u,j}$ = skor prediksi user $u$ terhadap item negatif $j$
* $i \in \text{positive items}$, $j \in \text{negative samples}$

WARP-KOS bekerja dengan men-sampling item negatif $j$ hingga ditemukan satu yang skor prediksinya lebih tinggi dari item positif $i$, lalu menghitung loss berdasarkan perbedaan skor.


#### Kelebihan

* Dapat memberikan rekomendasi untuk item baru (cold-start)
* Tidak bergantung pada interaksi antar pengguna
* Dapat menjelaskan alasan rekomendasi karena berdasarkan fitur nyata

#### Kekurangan

* Bergantung pada kualitas dan kelengkapan data konten
* Rentan terhadap overspecialization jika fitur terlalu terbatas

---
#### Top-5 Recommendation Output

> Correlative filtering model

|index|ISBN|Book-Title|Book-Author|Year-Of-Publication|Publisher|
|---|---|---|---|---|---|
|19|0452264464|Beloved \(Plume Contemporary Fiction\)|Toni Morrison|1994|Plume|
|101|0446677450|Rich Dad, Poor Dad: What the Rich Teach Their Kids About Money--That the Poor and Middle Class Do Not\!|Robert T\. Kiyosaki|2000|Warner Books|
|4729|068480087X|POSTCARDS|Annie Proulx|1994|Scribner|
|5027|0060086246|Last Chance Saloon|Marian Keyes|2003|Perennial|
|6145|0140181903|The Custom of the Country \(Penguin Twentieth-Century Classics\)|Edith Wharton|1990|Penguin Books|

> Content-based filtering model

|index|ISBN|Book-Title|Book-Author|Year-Of-Publication|Publisher|
|---|---|---|---|---|---|
|1495|0451523067|Emma|Jane Austen|1994|New Amer Library Classics|
|1574|0060929596|As Nature Made Him : The Boy Who Was Raised as a Girl|John Colapinto|2001|Perennial|
|3011|0811816583|New York Dogs|Andrea Mohin|1997|Chronicle Books Llc|
|4971|034545071X|Distant Shores|KRISTIN HANNAH|2002|Ballantine Books|
|6297|0373291108|Lady Of The Keep \(Harlequin Historical\)|Sharon Schulze|2000|Harlequin|

#### Tentang WARP-KOS

WARP-KOS merupakan loss function yang dirancang untuk memaksimalkan ranking item yang relevan. Loss ini bekerja dengan memilih item negatif secara acak hingga ditemukan satu yang lebih tinggi skornya daripada item positif, dan menghitung loss berdasarkan estimasi ranking tersebut. Loss ini sangat efisien untuk skenario top-N recommendation dan ranking-based learning.

---

### Evaluation

#### Precision\@K

$$\text{Precision@K} = \frac{|\text{Recommended items} \cap \text{Relevant items}|}{K}$$

**Keterangan:**

* $\text{Recommended items}$ = daftar item yang direkomendasikan oleh sistem
* $\text{Relevant items}$ = daftar item yang benar-benar relevan berdasarkan data ground truth
* $K$ = jumlah item rekomendasi teratas yang dievaluasi
* Precision\@K mengukur seberapa banyak dari $K$ rekomendasi yang benar-benar relevan

---

#### Recall\@K

$$\text{Recall@K} = \frac{|\text{Recommended items} \cap \text{Relevant items}|}{|\text{Relevant items}|}$$

**Keterangan:**

* $|\text{Relevant items}|$ = total jumlah item relevan untuk user tersebut
* Recall\@K mengukur seberapa banyak item relevan yang berhasil ditemukan dari seluruh item relevan yang tersedia

#### Hasil Evaluasi

> K = 2

| Model                                        | Precision\@K     | Recall\@K             |
| -------------------------------------------- |------------------|-----------------------|
| Collaborative Filtering (LightFM + WARP-KOS) | 0.58480823       | 0.7196675551073691    |
| Content-Based Filtering (LightFM + WARP-KOS) | 0.39675516       | 0.5046003379627569    |

> K = 5

| Model                                        | Precision\@K     | Recall\@K             |
| -------------------------------------------- |------------------|-----------------------|
| Collaborative Filtering (LightFM + WARP-KOS) | 0.38982305       | 0.9146288402013032    |
| Content-Based Filtering (LightFM + WARP-KOS) | 0.3060472        | 0.783271200892335     |

> K = 10

| Model                                        | Precision\@K     | Recall\@K             |
| -------------------------------------------- |------------------|-----------------------|
| Collaborative Filtering (LightFM + WARP-KOS) | 0.24410029       | 0.9611317684068246    |
| Content-Based Filtering (LightFM + WARP-KOS) | 0.21666665       | 0.9105260008583977    |


---------------

### Apakah sudah menjawab setiap problem statement?

Setiap pertannyaan pada problem statement telah terjawab, jika dilihat dari tabel hasil evaluasi dapat dinilai bahwa pendekatan menggunakan teknik collaborative filtering lebih unggul secara keseluruhan
pada saat k=2, k=5, dan k=10 jika dibandingkan dengan teknik content-based filtering. metrik yang digunakan untuk mengukur hasil evaluasi adalah Precision\@K dan Recall\@K.
model sistem rekomendasi dengan teknik content-based filtering memiliki beberapa kelebihan , yaitu unggul dalam memberikan rekomendasi untuk item baru , tidak bergantung kepada iteraksi antar pengguna.
Namun sangat tergantung oleh fitur dan kualitas dari datanya yang memnyebabkan rentan terhadap overspecialization. Di sisi lain model sistem rekomendasi dengan teknik correlative filtering tidak bergantung kepada fitur dari data serta
dan cocok apabila berfokus pada peningkatan rangking kepada item yang relevan namun kurang apabila ada user baru atau item baru serta sangat bergantung kepada data atau interaksi historis dari pelanggan.


### Apakah berhasil mencapai setiap goals yang diharapkan?

Semua poin dari goals atau tujuan sudah terpenuhi dengan baik. Model sistem rekomendasi berbasis teknik collaborative filtering dan content-based filtering berhasil dibuat menggunakan library LightFM dan diukur menggunakan metrik precision@K dan recall@K serta berdasarkan hasil evaluasi diketahuilah kelebihan serta kekurangan dari masing-masing model.

### Apakah setiap solusi statement yang kamu rencanakan berdampak? Jelaskan!

Sangat membantu dan berdampak. pembuatan model sistem rekomendasi menggunakan dua teknik yang berbeda yaitu collaborative filterin dan content-based filtering dapat
dijadikan sebagai pertimbangan untuk menerapkan model berdasarkan kasus-kasus yang berbeda, efektivitas dari model dipengaruhi oleh beberapa faktor dan setiap model memiliki
kelebihan serta kekurangannya masing-masing. pengukuran efektivitas model menggunakan metrik pengukuran seperti precision@k dan recall@k memungkinkan dalam menentukan performa dari model namun
tidak mencerminkan efektivitas dari model secara gamblang walaupun model sistem rekomendasi menggunakan teknik collaborative filtering terlihat lebih baik dibandingkan dengan
sistem rekomendasi menggunakan teknik content-based filtering jika diukur menggunakan metrik precision@k dan recall@k.


"""